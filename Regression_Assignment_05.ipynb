{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-01    What is Elastic Net Regression and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Elastic Net Regression` : Combining Strengths for Better Models**\n",
    "\n",
    "It's a **regularized regression** technique that combines the strengths of both Lasso and Ridge Regression. Regularization penalizes complex models to prevent overfitting and improve generalizability. \n",
    "\n",
    "*    **`Here's how Elastic Net works` :**\n",
    "\n",
    "        1. **Combines penalties:** It adds a penalty term to the standard least squares objective function, combining the L1 penalty from Lasso (enforces sparsity) and the L2 penalty from Ridge (shrinks coefficients).\n",
    "\n",
    "        2. **Feature selection and shrinkage:** Similar to Lasso, it can set some coefficients to zero, effectively removing irrelevant features. However, unlike Lasso, it doesn't necessarily pick only one feature from a group of correlated ones.\n",
    "\n",
    "        3. **Balances sparsity and stability:** By combining both penalties, Elastic Net offers a balance between feature selection and coefficient shrinkage, leading to potentially better model performance and interpretability.\n",
    "\n",
    "*    **`How does it differ from other techniques` :**\n",
    "\n",
    "        * **Lasso -**\n",
    "            \n",
    "            * Enforces sparsity by setting some coefficients to zero, leading to feature selection.\n",
    "    \n",
    "            * Can be unstable with highly correlated features, often selecting only one from a group.\n",
    "        \n",
    "        * **Ridge -**\n",
    "        \n",
    "            * Shrinks all coefficients towards zero, reducing variance but not necessarily leading to feature selection.\n",
    "    \n",
    "            * More stable than Lasso but might not remove irrelevant features.\n",
    "\n",
    "        * **Elastic Net -**\n",
    "    \n",
    "            * Combines the benefits of both, offering **sparsity and stability**.\n",
    "    \n",
    "            * Can handle correlated features better than Lasso, potentially selecting multiple relevant features from a group.\n",
    "\n",
    "*    **`Advantages of Elastic Net Regression` :**\n",
    "\n",
    "        * **Improved model performance:** By addressing overfitting and potentially selecting relevant features, it can lead to better prediction accuracy compared to Lasso or Ridge alone.\n",
    "\n",
    "        * **Feature selection:** Similar to Lasso, it can help identify important features for interpretability.\n",
    "        \n",
    "        * **Robustness to multicollinearity:** Handles correlated features better than Lasso, potentially leading to more stable models.\n",
    "\n",
    "*    **`Disadvantages of Elastic Net Regression` :**\n",
    "\n",
    "        * **Tuning additional parameter:** Requires tuning both the L1 and L2 penalty parameters, which can be more complex than tuning a single parameter in Lasso or Ridge.\n",
    "\n",
    "        * **Interpretability:** While it can perform feature selection, the interpretation of coefficients might be less straightforward compared to models without shrinkage.\n",
    "\n",
    "**In summary, Elastic Net Regression offers a valuable alternative to Lasso and Ridge Regression, especially when dealing with high-dimensional data, correlated features, and the need for both feature selection and model stability.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-02    How do you choose the optimal values of the regularization parameters for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the optimal values for the regularization parameters in Elastic Net Regression is crucial for achieving good performance and avoiding overfitting.**\n",
    "\n",
    "**There are two main parameters to consider :**\n",
    "\n",
    "1. **$λ$ (lambda):** This controls the overall amount of regularization applied to the model. Higher values of λ lead to stronger regularization, potentially reducing model complexity but also increasing bias.\n",
    "\n",
    "2. **$α$ (alpha):** This parameter mixes between L1 (Lasso) and L2 (Ridge) regularization. When α = 0, it becomes Ridge regression, and when α = 1, it becomes Lasso regression. Values between 0 and 1 create a blend of both.\n",
    "\n",
    "**Here are two common approaches to choose the optimal values:**\n",
    "\n",
    "1. **`Grid Search with Cross-validation` -**\n",
    "\n",
    "    * Define a grid of possible values for both λ and α.\n",
    "\n",
    "    * For each combination of λ and α, split the data into training and validation sets.\n",
    "\n",
    "    * Train the Elastic Net model on the training set with the specific λ and α values.\n",
    "\n",
    "    * Evaluate the model performance on the validation set using a metric like mean squared error (MSE) or R-squared.\n",
    "\n",
    "    * Repeat for all combinations of λ and α in the grid.\n",
    "\n",
    "    * Choose the combination of λ and α that results in the best performance on the validation set.\n",
    "\n",
    "2. **`Nested Cross-validation` -**\n",
    "\n",
    "    * This approach involves two levels of cross-validation:\n",
    "\n",
    "        * **Outer loop:** Splits the data into outer folds.\n",
    "\n",
    "        * **Inner loop:** For each outer fold:\n",
    "\n",
    "            * Further split the data within the fold into inner folds.\n",
    "\n",
    "            * Perform grid search with cross-validation as described above to find the optimal λ and α for the inner folds.\n",
    "\n",
    "            * Use the chosen λ and α to train a model on the entire inner training set and evaluate its performance on the inner validation set.\n",
    "\n",
    "        * Average the performance metric across all outer folds.\n",
    "\n",
    "        * Choose the λ and α combination that leads to the best average performance across all outer folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-03    What are the advantages and disadvantages of Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Advantages of Elastic Net Regression` :**\n",
    "\n",
    "* **Addresses Multicollinearity:** Unlike Lasso, which can struggle with highly correlated features, Elastic Net groups correlated features and selects the most informative ones, leading to better model stability and avoiding arbitrary feature selection.\n",
    "\n",
    "* **Effective Feature Selection:** Similar to Lasso, Elastic Net shrinks coefficients towards zero, potentially setting some to zero entirely. This effectively performs feature selection, resulting in a simpler model with fewer features, improving interpretability and reducing overfitting.\n",
    "\n",
    "* **Balances Bias-Variance Trade-off:** By combining L1 and L2 regularization, Elastic Net offers a better balance between bias and variance compared to either Lasso or Ridge regression. This can lead to improved prediction performance in certain scenarios.\n",
    "\n",
    "* **Handles High Dimensionality:** Elastic Net is well-suited for datasets with many features, even when the number of observations is relatively small. This makes it valuable for modern datasets with numerous potential influencing factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Disadvantages of Elastic Net Regression` :**\n",
    "\n",
    "* **Increased Computational Cost:** Compared to Lasso or Ridge regression, Elastic Net requires more computational resources due to its dual regularization nature and the need for hyperparameter tuning.\n",
    "\n",
    "* **Potential Loss of Predictive Power:** When features are not correlated or the number of features is much smaller than observations, Elastic Net might unnecessarily shrink coefficients, leading to reduced predictive power or introducing bias.\n",
    "\n",
    "* **Reduced Interpretability:** While feature selection is advantageous, it can also make interpreting the model more complex, especially when many features have small coefficients or only a few have large coefficients.\n",
    "\n",
    "* **Hyperparameter Tuning Complexity:** Tuning the two regularization parameters (alpha and lambda) in Elastic Net can be more challenging compared to single-parameter methods like Lasso or Ridge, requiring careful consideration and potentially more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-04    What are some common use cases for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Elastic Net Regression`, a powerful regression technique, finds applications in various domains due to its unique capabilities.**\n",
    "\n",
    "**`Here are some of its most common use cases` :**\n",
    "\n",
    "1. **Variable Selection and Model Interpretability -**\n",
    "\n",
    "    * **High-dimensional data :** When dealing with datasets containing numerous features, potentially exceeding the number of observations, Elastic Net performs **automatic variable selection**. It shrinks coefficients of irrelevant or redundant features to zero, effectively removing them from the model. This leads to a **sparser** and **more interpretable** model, highlighting the key factors influencing the outcome.\n",
    "\n",
    "2. **Handling Multicollinearity -**\n",
    "\n",
    "    * **Correlated features :** When features within a dataset are highly correlated (multicollinearity), traditional regression methods can suffer from instability and unreliable coefficient estimates. Elastic Net addresses this by combining L1 and L2 regularization. The L1 penalty encourages sparsity, driving coefficients of irrelevant features to zero, while the L2 penalty helps reduce variance and improve model stability even in the presence of correlated features.\n",
    "\n",
    "3. **Risk Prediction and Survival Analysis -**\n",
    "\n",
    "    * **Medical research :** In fields like healthcare, Elastic Net is employed for tasks like **cancer prognosis**, **disease risk prediction**, and **patient survival analysis**. By identifying relevant factors from complex medical data, it helps healthcare professionals make informed decisions and personalize treatment strategies.\n",
    "\n",
    "4. **Other Applications -**\n",
    "\n",
    "    * **Finance :** Elastic Net finds use in **portfolio optimization**, selecting assets that maximize returns while minimizing risk.\n",
    "        \n",
    "    * **Marketing :** It can be used for **customer segmentation** and **churn prediction**, aiding in targeted marketing campaigns and customer retention strategies.\n",
    "        \n",
    "    * **Social Sciences :** Researchers utilize Elastic Net to analyze social and economic data, identifying factors influencing various social phenomena.\n",
    "\n",
    "Overall, Elastic Net Regression offers a valuable tool for various situations where data analysis requires **variable selection**, **handling multicollinearity**, and building **interpretable models** from potentially complex datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-05    How do you interpret the coefficients in Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting coefficients in Elastic Net Regression can be a bit more nuanced compared to standard linear regression due to its regularization properties. \n",
    "\n",
    "**`Here's a breakdown` :**\n",
    "\n",
    "*    **General Interpretation -**\n",
    "\n",
    "        * **Magnitude:** Similar to linear regression, the **absolute value** of a coefficient reflects the **strength** of the relationship between the corresponding feature and the target variable. A larger absolute value indicates a stronger impact.\n",
    "        \n",
    "        * **Direction:** The **sign** of the coefficient indicates the **direction** of the effect. A positive coefficient suggests a positive relationship (increasing feature value leads to increasing target value), while a negative coefficient suggests a negative relationship.\n",
    "        \n",
    "        * **Feature Selection:** Unlike standard regression, Elastic Net can **shrink coefficients to zero**, effectively **removing** those features from the model. This feature selection aspect helps combat overfitting and identify relevant features.\n",
    "\n",
    "*    **Impact of Regularization -**\n",
    "\n",
    "        * **Regularization parameters:** Elastic Net combines penalties from both Ridge and Lasso regressions, controlled by two parameters: **lambda (λ)** and **alpha (α)**.\n",
    "        \n",
    "        * **Lambda (λ):** This shrinks all coefficients towards zero, reducing their magnitude and potentially leading to some becoming zero for smaller values.\n",
    "        \n",
    "        * **Alpha (α):** This encourages sparsity by driving some coefficients exactly to zero, similar to Lasso.\n",
    "\n",
    "*    **Interpreting in Context -**\n",
    "\n",
    "        * **Compare coefficients:** While the magnitude of coefficients can suggest relative importance, it's crucial to consider the **standardized coefficients** or **feature importances** provided by some algorithms. These account for different feature scales and provide a more reliable comparison.\n",
    "        \n",
    "        * **Coefficient paths:** Plotting coefficients against different values of lambda or alpha can be helpful. This visualizes how regularization affects their values and helps identify the optimal settings where coefficients are stable and significant.\n",
    "        \n",
    "        * **Remember:** Coefficients in Elastic Net **don't directly translate to feature importance** due to regularization. They primarily reflect the **adjusted linear relationship** between features and the target variable after considering the model's complexity.\n",
    "\n",
    "*    **Additional Points -**\n",
    "\n",
    "        * **Focus on non-zero coefficients:** As some coefficients might be shrunk to zero, only interpret those that remain after model fitting.\n",
    "\n",
    "        * **Combine with other techniques:** Interpreting coefficients alongside feature importance measures or model visualizations can provide a more comprehensive understanding of feature relevance.\n",
    "\n",
    "By understanding these aspects, you can effectively interpret coefficients in Elastic Net Regression and gain valuable insights into the relationships between features and the target variable in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-06    How do you handle missing values when using Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values is crucial before applying Elastic Net Regression, as it can significantly impact the model's performance and stability. Here are some common approaches:\n",
    "\n",
    "1. **`Imputation` :** This involves replacing missing values with estimates based on other available data points. Several imputation techniques exist, each with its own advantages and disadvantages:\n",
    "\n",
    "     * **Mean/Median/Mode imputation:** Replaces missing values with the mean, median, or mode of the corresponding feature, respectively. Simple and fast, but may not capture the underlying distribution of the data.\n",
    "\n",
    "     * **K-Nearest Neighbors (KNN):** Imputes missing values based on the values of the k nearest neighbors in the training data. More sophisticated than simple imputation methods, but requires choosing the appropriate value for k.\n",
    "        \n",
    "     * **Model-based imputation:** Uses statistical models like linear regression or decision trees to predict missing values based on other features. More flexible than simpler methods, but requires careful model selection and evaluation.\n",
    "\n",
    "2. **`Deletion` :** This involves removing observations with missing values entirely. This approach is straightforward but can lead to data loss, especially if missingness is widespread.\n",
    "\n",
    "3. **`Feature engineering` :** In some cases, you can create new features based on existing ones to handle missing values. For example, you could create a binary feature indicating whether a value is missing or not.\n",
    "\n",
    "*    **Choosing the best approach depends on several factors -**\n",
    "\n",
    "* **The amount and pattern of missing data:** Randomly missing values may be handled differently than systematically missing ones.\n",
    "* **The nature of the features:** Continuous features might be imputed differently than categorical ones.\n",
    "* **The desired properties of the model:** Some techniques may be more suitable for preserving interpretability, while others prioritize accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-07    How do you use Elastic Net Regression for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Regression is a powerful technique for both **regression and feature selection**. It combines the strengths of Lasso and Ridge regression, offering several advantages :**\n",
    "\n",
    "1. **Shrinking coefficients -** Similar to Lasso regression, Elastic Net shrinks the coefficients of irrelevant features towards zero. If a coefficient becomes exactly zero, it effectively removes the corresponding feature from the model. This leads to a **sparser model** with fewer features, improving interpretability and reducing overfitting.\n",
    "\n",
    "2. **Handling correlated features -** Unlike Lasso, which can arbitrarily select one feature from a group of highly correlated features, Elastic Net incorporates an L2 penalty that encourages **coefficient shrinkage across all features**. This helps to address multicollinearity and improve model stability.\n",
    "\n",
    "**Here's `how Elastic Net achieves feature selection` :**\n",
    "\n",
    "a. **L1 penalty -** The L1 penalty, also known as the Lasso penalty, encourages sparsity by adding the absolute value of each coefficient to the cost function. Features with small contributions have their coefficients shrink towards zero, and coefficients that reach zero effectively remove the feature from the model.\n",
    "\n",
    "b. **L2 penalty -** The L2 penalty, also known as the Ridge penalty, penalizes the sum of squared coefficients. This helps to **shrink all coefficients** towards zero, even if they are not driven to zero by the L1 penalty. This promotes stability and reduces the impact of correlated features.\n",
    "\n",
    "c. **Mixing parameter -** Elastic Net introduces a **mixing parameter (l1_ratio)** that controls the relative contribution of the L1 and L2 penalties. A higher l1_ratio emphasizes sparsity and feature selection, while a lower value focuses more on coefficient shrinkage and stability.\n",
    "\n",
    "**`Summary` :**\n",
    "\n",
    "* By combining the L1 and L2 penalties, Elastic Net performs **both feature selection and coefficient shrinkage**.\n",
    "\n",
    "* Features with minimal contribution have their coefficients driven to zero by the L1 penalty, effectively removing them from the model.\n",
    "\n",
    "* The L2 penalty helps to address multicollinearity and improve model stability.\n",
    "\n",
    "* The mixing parameter allows you to control the balance between sparsity and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-08    How do you pickle and unpickle a trained Elastic Net Regression model in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can pickle and unpickle a trained Elastic Net Regression model in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Importing libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load Dataset, Drop Feature and Encode Categorical Feature :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Step 1. Load dataset -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Rain</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>BUI</th>\n",
       "      <th>FWI</th>\n",
       "      <th>Classes</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>64.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>13.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year  Temperature  RH  Ws  Rain  FFMC  DMC    DC  ISI  BUI  \\\n",
       "0    1      6  2012           29  57  18   0.0  65.7  3.4   7.6  1.3  3.4   \n",
       "1    2      6  2012           29  61  13   1.3  64.4  4.1   7.6  1.0  3.9   \n",
       "2    3      6  2012           26  82  22  13.1  47.1  2.5   7.1  0.3  2.7   \n",
       "3    4      6  2012           25  89  13   2.5  28.6  1.3   6.9  0.0  1.7   \n",
       "4    5      6  2012           27  77  16   0.0  64.8  3.0  14.2  1.2  3.9   \n",
       "\n",
       "   FWI      Classes  Region  \n",
       "0  0.5  not fire          0  \n",
       "1  0.4  not fire          0  \n",
       "2  0.1  not fire          0  \n",
       "3  0.0  not fire          0  \n",
       "4  0.5  not fire          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv('Algerian_forest_fires_cleaned_dataset.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Step 2. Drop 'Day', 'Month' and 'Year' -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Rain</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>BUI</th>\n",
       "      <th>FWI</th>\n",
       "      <th>Classes</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>64.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>13.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  RH  Ws  Rain  FFMC  DMC    DC  ISI  BUI  FWI      Classes  \\\n",
       "0           29  57  18   0.0  65.7  3.4   7.6  1.3  3.4  0.5  not fire      \n",
       "1           29  61  13   1.3  64.4  4.1   7.6  1.0  3.9  0.4  not fire      \n",
       "2           26  82  22  13.1  47.1  2.5   7.1  0.3  2.7  0.1  not fire      \n",
       "3           25  89  13   2.5  28.6  1.3   6.9  0.0  1.7  0.0  not fire      \n",
       "4           27  77  16   0.0  64.8  3.0  14.2  1.2  3.9  0.5  not fire      \n",
       "\n",
       "   Region  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop(['day','month','year'],axis=1,inplace=True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Step 3.  Encode Categorical Feature -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Rain</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>BUI</th>\n",
       "      <th>FWI</th>\n",
       "      <th>Classes</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>64.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>13.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  RH  Ws  Rain  FFMC  DMC    DC  ISI  BUI  FWI  Classes  Region\n",
       "0           29  57  18   0.0  65.7  3.4   7.6  1.3  3.4  0.5        0       0\n",
       "1           29  61  13   1.3  64.4  4.1   7.6  1.0  3.9  0.4        0       0\n",
       "2           26  82  22  13.1  47.1  2.5   7.1  0.3  2.7  0.1        0       0\n",
       "3           25  89  13   2.5  28.6  1.3   6.9  0.0  1.7  0.0        0       0\n",
       "4           27  77  16   0.0  64.8  3.0  14.2  1.2  3.9  0.5        0       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Classes']=np.where(df['Classes'].str.contains(\"not fire\"),0,1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training the model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Rain</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>BUI</th>\n",
       "      <th>Classes</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>64.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>13.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  RH  Ws  Rain  FFMC  DMC    DC  ISI  BUI  Classes  Region\n",
       "0           29  57  18   0.0  65.7  3.4   7.6  1.3  3.4        0       0\n",
       "1           29  61  13   1.3  64.4  4.1   7.6  1.0  3.9        0       0\n",
       "2           26  82  22  13.1  47.1  2.5   7.1  0.3  2.7        0       0\n",
       "3           25  89  13   2.5  28.6  1.3   6.9  0.0  1.7        0       0\n",
       "4           27  77  16   0.0  64.8  3.0  14.2  1.2  3.9        0       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.4\n",
       "2    0.1\n",
       "3    0.0\n",
       "4    0.5\n",
       "Name: FWI, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Independent Feature\n",
    "X=df.drop('FWI',axis=1)\n",
    "display(X.head())\n",
    "\n",
    "# Dependent Feature\n",
    "y=df['FWI']\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Feature Selection :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BUI', 'DC'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((182, 9), (61, 9))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "## threshold--Domain expertise\n",
    "corr_features=correlation(X_train,0.85)\n",
    "display(corr_features)\n",
    "\n",
    "## drop features when correlation is more than 0.85\n",
    "X_train.drop(corr_features,axis=1,inplace=True)\n",
    "X_test.drop(corr_features,axis=1,inplace=True)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Feature Scaling Or Standardization :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Elasticnet Regression :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error 1.8822353634896005\n",
      "R2 Score 0.8753460589519703\n"
     ]
    }
   ],
   "source": [
    "elastic=ElasticNet()\n",
    "elastic.fit(X_train_scaled,y_train)\n",
    "y_pred=elastic.predict(X_test_scaled)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "score=r2_score(y_test,y_pred)\n",
    "print(\"Mean absolute error\", mae)\n",
    "print(\"R2 Score\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Pickling the model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ElasticNet_model.pkl\", \"wb\") as f:\n",
    "    # Pickle the model using pickle.dump\n",
    "    pickle.dump(elastic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Unpickling the model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ElasticNet_model.pkl\", \"rb\") as f:\n",
    "    # Load the model using pickle.load\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Explanation` :**\n",
    "\n",
    "* We import the necessary libraries: `pickle` for serialization and `ElasticNet` from `sklearn.linear_model` for the model.\n",
    "\n",
    "* We train the Elastic Net model with your data and desired hyperparameters.\n",
    "\n",
    "* In pickling, we open a file in binary write mode (`\"wb\"`) and use `pickle.dump` to serialize the trained model (`model`) into the file.\n",
    "\n",
    "* In unpickling, we open the pickled file in binary read mode (`\"rb\"`) and use `pickle.load` to deserialize the model data and store it in the `loaded_model` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-09    What is the purpose of pickling a model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In machine learning, pickling serves a crucial purpose: `saving and reusing trained models` efficiently.** \n",
    "\n",
    "**`Here's why it's valuable` :**\n",
    "\n",
    "1. **Avoiding Re-training -** Training machine learning models can be computationally expensive, taking hours or even days depending on the dataset and model complexity. Pickling allows you to save the trained model in a file after the training process. This way, you can reload and use the model for predictions on new data without re-training it from scratch, saving significant time and resources.\n",
    "\n",
    "2. **Sharing and Collaboration -** Pickled models can be easily shared with other data scientists or deployed in production environments. This facilitates collaboration and allows others to use the model for their own purposes without needing access to the original training data or code.\n",
    "\n",
    "3. **Version Control and Experimentation -** By pickling different versions of your model after training with various parameters or datasets, you can easily compare their performance and track progress over time. This enables effective model selection and experimentation.\n",
    "\n",
    "4. **Continuous Integration and Deployment -** In production settings, pickling allows you to integrate trained models into continuous integration and deployment pipelines. This streamlines the process of deploying and updating models in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
